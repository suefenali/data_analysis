{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle report\n",
    "\n",
    "#### by Suelen Fenali\n",
    "\n",
    "This wrangling project consisted mainly of three necessary steps to prepare the data for the analysis and visualizations.\n",
    "\n",
    "The first step was gathering the data. There were three different sources from which data should be gathered. A csv file provided inside the project description and accessible through download; a tsv file available in the Udacity's server that should be collected using the python library `requests`; and Twitter data from the WeRateDogs timeline, which were provided through an API called tweepy. Gathering the data from the two first sources was pretty straightforward, but from the API was a little bit more complicated. It took me two days to get access to Twitter's developer mode, after that I created an app, and was able to use my keys to get more data to enrich the tweets. After running the code it took 30 minutes (1847 seconds) to get data for 2356 tweets. I saved the data to a json file, and then read it line by line, and the first step was complete. I thought at the begging that it would be difficult to finish, but it was actually really fun and did not take too much time.\n",
    "\n",
    "The second step was assessing the data. I used first the visual method, and just taking a look at the data some problems could be already identified, such as: missing data, inaccurate data some dog's names, and tidiness issues as well. While I was seeing the issues I was documenting it at the end of the section. During the programmatic assessment I could find that some columns had wrong data types and others had more missing data. At the end of this step, I organized my notes into quality and tidiness issues for each DataFrame that was assessed.\n",
    "\n",
    "The third and final step was cleaning the data according to what I found during the `assess` part. I saw that fixing some tidiness issues would fix missing issues as well, so I started doing so. Cleaning the data is my favorite moment, but I think it was the biggest time effort. I did some iterations during this step, because while cleaning I realized some points that I have not seen before, for example, that I could have one boolean column to check if the tweet was a reply. I went back to the assess notes and created one more issue to make the report consistent.\n",
    "\n",
    "The way your data is prepared to the analysis plays a great part during the whole process. Gathering, assessing and cleaning can be not the exciting tasks for everyone, but I believe that a well treated data helps a lot to create a good EDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
